# Modelos de lenguaje y n-gramas

Los **modelos de lenguaje** son
una parte fundamental de varias tareas de NLP (procesamiento de
lenguaje natural), como reconocimiento de lenguaje hablado, 
reconocimiento de lenguaje escrito, traducción automática, 
corrección de ortografía, sistemas de predicción de escritura,
etc. (ver [@jurafsky], capítulo 4, o en la nueva edición
el [capítulo 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf)
).


```{block2, type ='resumen'}
Un **modelo del lenguaje** de tipo estadístico es una asignación de 
probabilidades $P(W)$ a cada posible frase del lenguaje
$W = w_1 w_2 w_3 \cdots w_n$. 

```

Estos modelos del lenguaje están diseñados para
resolver distintas tareas particulares y estimar probabilidades particulares
de interés, de modo que hay muchas maneras de entrenar estos modelos.

Comenzaremos por entender métodos básicos para construir estos modelos,
como conteo de *n-gramas*, y consideraremos métodos modernos que se basan
en *representaciones distribuidas*.


## Ejemplo: Modelo de canal ruidoso

El modelo del canal ruidoso muestra una situación general en la que los modelos
del lenguaje son importantes:

```{block2, type='resumen'} 
**Canal ruidoso**
  
En el modelo del canal ruidoso tratamos mensajes recibidos como si fueran *distorsionados* o transformados al pasar por un canal de comunicación ruidoso (por ejemplo, escribir en el celular).

La tarea que queremos resolver con este modelo es 
**inferir** la palabra o texto correctos a partir de 

- Un modelo de distorsión o transformación (modelo del canal)
- Un modelo del lenguaje.
```

Ahora veremos por qué necesitamos estas dos partes. Supongamos que recibimos 
un mensaje codificado o transformado $X$ (quizá texto con errores,
o sonido, o una página escrita), y quisiéramos recuperar el mensaje original $W$
(sucesión de palabras en texto). Este problema lo podemos enfocar como sigue:

- Quisiéramos calcular, para cada mensaje **en texto** $W$ la probabilidad

$$P(W|X).$$

Propondríamos entonces como origen el texto $W^*$ que maximiza esta probabilidad condicional:

$$W^* = argmax_W P(W|X),$$

que en principio es un máximo sobre todas las posibles frases del lenguaje.

¿Cómo construimos esta probabilidad condicional? Tenemos que
para cada posible frase $W$,

$$P(W|X) = \frac{P(X|W)P(W)}{P(X)},$$

así que podemos escribir ($X$ es constante)

$$ W^* = argmax_W P(X|W)P(W).$$

Esta forma tiene dos partes importantes:

1. **Verosimilitud**: la probabilidad $P(X|W)$ de observar el mensaje transfromado $X$ dado que el mensaje es $W$. Este es el  **modelo del canal** (o **modelo de errores**), que nos dice cómo ocurren errores o transformaciones $X$ cuando se pretende comunicar el mensaje $W$.

2. **Inicial** o **previa**: La probabilidad $P(W)$ de observar el mensaje $W$ en el contexto actual. Esto depende más del lenguaje que del canal, y le llamamos el **modelo del lenguaje**.

3. Nótese que con estas dos partes tenemos un modelo generativo para mensajes del canal ruidoso: primero seleccionamos un texto mediante el modelo de lenguaje, con las probabilidades $P(W)$, y dado el mensaje
construimos el mensaje recibido según las probabilidades $P(X|W)$.

#### Ejemplos {-}

Supongamos que recibimos el mensaje $X=$"Estoy a días minutos", 
y supongamos que tenemos tres frases en nuestro lenguaje:

$W_1=$"Estoy a veinte minutos", $W_2=$"Estoy a diez minutos", y $W_3$= "No voy a llegar", y $W_4$="Estoy a tías minutos". Supongamos que las probabilidades de cada una, dadas por el modelo de lenguaje, son
(independientemente del mensaje recibido):

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
p <- tibble(W = c("$W_1$","$W_2$", "$W_3$", "$W_4$"),
                `P(W)` = c(0.001, 0.001, 0.008, 0.000001)) 
p %>% knitr::kable(format = 'markdown')
```

Ahora supongamos que el modelo del canal (digamos que sabemos el mecanismo mediante el cual se
escriben los mensajes de texto) nos da:

```{r, echo=FALSE}
p <- tibble(W = c("$W_1$","$W_2$", "$W_3$", "$W_4$"),
                `P(W)` = c(0.001, 0.001, 0.008, 0.000001),
                `P(X|W)` = c(0.01, 0.12, 0, 0.05)) 
p %>% knitr::kable(format = 'markdown')
```

Obsérvese que la probabilidad condicional más alta es la segunda, y en
este modelo es imposible que $W_3$ se transforme en $X$ bajo el canal ruidoso. Multiplicando estas dos probabilidades obtenemos:

```{r, echo=FALSE}
p <- tibble(W = c("$W_1$","$W_2$", "$W_3$", "$W_4$"),
                `P(W)` = c(0.002, 0.001, 0.008, 1e-6),
                `P(X|W)` = c(0.01, 0.12, 0, 0.05)) %>%
  mutate(`P(X|W)P(W)` = `P(W)` * `P(X|W)`)
p %>% knitr::kable(format = 'markdown')
```
de modo que escogeríamos la segunda frase (máxima probabilidad condicional) como la interpretación de
"Estoy a días minutos".

Nótese que $P(X|W_3)$ en particular es muy bajo,
porque es poco posible que el canal distosione "No voy a llegar" 
a "Estoy a días minutos". Por otro lado $P(W_4)$ también
es muy bajo, pues la frase $W_4$ tiene probabilidad muy baja de 
ocurrir.

#### Ejercicio {-}

Piensa cómo sería el modelo de canal ruidoso $P(X|W)$ si nuestro problema fuera reconocimiento de frases habladas o escritas, o en traducción entre dos lenguajes.


## Corpus y vocabulario

En  los ejemplos vistos arriba, vemos que necesitamos definir qué son
las *palabras* $w_i$, qué lenguaje estamos considerando, y cómo estimamos las probabilidades.

```{block2, type='comentario'}
- Un **corpus** es una colección de textos (o habla) del lenguaje que nos interesa.
- El **vocabulario** es una colección de palabras que ocurren en el **corpus** (o más general, en el lenguaje).
- La definición de **palabra** (token) depende de la tarea de NLP que nos interesa.
```

Algunas decisiones que tenemos que tomar, por ejemplo:

- Generalmente, cada palabra está definida como una unidad separada
por espacios o signos de puntuación.
- Los signos de puntuación pueden o no considerarse como palabras.
- Pueden considerarse palabras distintas las que tienen mayúscula y las que no (por ejemplo, para reconocimiento de lenguaje hablado no nos
interesan las mayúsculas).
- Pueden considerarse palabras distintas las que están escritas incorrectamente, o no.
- Pueden considerarse plurales como palabras distintas, formas en masculino/femenino, etc. (por ejemplo, en clasificación de textos quizá
sólo nos importa saber la raíz en lugar de la forma completa de la palabra).
- Comienzos y terminación de oraciones pueden considerarse como
"palabras" (por ejemplo, en reconocimiento de texto hablado).

```{block2, type='resumen'}
Al proceso que encuentra todas las palabras en un texto se le
llama **tokenización** o **normalización de texto**. Los **tokens**
de un texto son las ocurrencias en el texto de las palabras
en el vocabuario.
```

En los ejemplos que veremos a continuación consideraremos las
siguiente normalización:

- Consideramos como una palabra el comienzo y el fin de una oración.
- Normalizamos el texto a minúsculas.
- No corregimos ortografía, y consideramos las palabras en la forma que ocurren.
- Consideramos signos de puntuación como palabras.

El **vocabulario** es el conjunto de todas las palabras posibles en nuestros
textos. Lo denotaremos como $w^1, w^2, \ldots, w^N$, para un vocabulario de $N$ 
distintas palabras o tokens.

## Modelos de lenguaje: n-gramas

Consideremos cómo construiríamos las probabilidades $P(W)$, de manera que
reflejen la ocurrencia de frases en nuestro lenguaje. Escribimos
$$W=w_1 w_2 w_3 \cdots w_n,$$ 
donde $w_i$ son las palabras que contienen el texto $W$.

Aquí nos enfrentamos al primer problema:

- Dada la variedad de frases que potencialmente hay en el lenguaje,
no tiene mucho sentido intentar estimar o enumerar
directamente estas probabilidades. Por ejemplo, si intentamos
algo como intentar ver una colección de ejemplos del lenguaje,
veremos que el número de frases es muy grande, y la mayor
parte de los textos o frases posibles en el lenguaje no
ocurren en nuestra colección (por más grande que sea la
colección).

Para tener un acercamiento razonable, necesitamos considerar
un modelo $P(W)$ con más estructura o supuestos. Hay varias maneras de hacer  esto, 
y un primer acercamiento
consiste en considerar solamente *el contexto más inmediato* de 
cada palabra. Es decir, la probabilidad de ocurrencia de palabras en una frase 
generalmente se puede evaluar con un contexto relativamente
chico (palabras cercanas) y no es necesario considerar la frase completa.

Consideramos entonces la regla del producto:

$$P(w_1 w_2 w_3 \cdots w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1 w_2) \cdots
P(w_n|w_1 w_2 w_3 \cdots w_{n-1})$$
Y observamos entonces que basta con calcular las probabilidades
condicionales de la siguiente palabra:
$$P(w_{m+1}|w_1\cdots w_m)$$
para cualquier conjunto de palabras $w,w_1,\ldots, w_m$. 

A estas $w,w_1,\ldots, w_m$ palabras que ocurren justo antes de $w$ les
llamamos el *contexto* de $w$ en la frase. Por la regla
del producto, podemos ver entonces nuestro problema como uno
de **predecir la palabra siguiente**, dado el contexto. Por ejemplo,
si tenemos la frase "como tengo examen entonces voy a ....", la probabilidad
de observar "estudiar" o "dormir" debe ser más alta que la de "gato".

```{block2, type='resumen'}
- A una sucesión de longitud $n$ de palabras $w_1w_2\cdots w_n$ le
llamamos un **n-grama** de nuestro lenguaje.
```


Igualmente, calcular todas estas condicionales contando tampoco es factible,
pues si el vocabulario es de tamaño $V$, entonces los contextos
posibles son de tamaño $V^m$, el cual es un número muy grande
incluso para $m$ no muy grande. Pero
podemos hacer una simplificación: suponer
que la predicción será suficientemente buena si limitamos el contexto de la siguiente
palabra a $n$-gramas, para una $n$ relativamente chica.

Por ejemplo, para **bigramas**, solo nos interesa calcular
$$P(w_m|w_{m-2}),$$

la dependencia hacia atrás contando solo la palabra anterior. Simplificaríamos la formula
de arriba como sigue:

$$P(w_1 w_2 w_3 \cdots w_n) = P(w_1)P(w_2|w_1)P(w_3|w_2) P(w_4|w_3)\cdots
P(w_n| w_{n-1})$$

Este se llama modelo basado en **bigramas**. En el caso más simple, establecemos que 
la ocurrencia de una palabra es independiente de su contexto:

$$P(w|w_1\cdots w_{n-1}) = P(w)$$

de forma que el modelo del lenguaje se simplifica a:

$$P(w_1 w_2 w_3 \cdots w_n) = P(w_1)P(w_2)P(w_3) \cdots P(w_n)$$

A este modelo le llamamos el modelo de **unigramas**.

### Modelo generativo de n-gramas

Para que estos modelos den realmente una distribución de probabilidad
sobre todas las frases posibles, es necesario tener también un modelo de la longitud de las frases.
Una manera es definir $P(N)$, que es la distribución sobre la longitud de frase. Sin embargo,
la manera más común es introducir dos símbolos nuevos

- Para inicio de frase usamos $<s>$
- Para fin de frase usamos $</s>$



De esta manera, el proceso de generación de frases en unigramas es el siguiente:

1. Comenzamos con el símbolo $<s>$. 

Comenzando con $i = 1$:

2. Escogemos una palabra $w_i$ del vocabulario $w^1$ a $w^N$ y $</s>$. Estas probabilidades las denotamos como
$P(w^1), P(w^2),\ldots, P(w^N)$ y $P(<\s>)$, y todas estas probabilides deben sumar 1.
3. Si escogimos $</s>$, terminamos la frase. En otro caso, regresamos a 2 con $i=i+1$.

La probabilidad de escoger la frase $P(<s>w_1w_2\cdots w_n</s>)$ es entonces

$P(<s>w_1w_2\cdots w_n</s>) = P(w_1)P(w_2)\cdots P(w_n)P(</s>).$

Por construcción, las probabilidades sobre todas las frases suman 1. Necesitamos definir
todas las probabilidades
$$P(w)$$
donde $w$ es una palabra o $</s>$.

#### Ejercicio
Supón que solo hay dos palabras en nuestro vocabulario $a$ y $b$. Calcula cuál es la probabilidad
de obtener una frase de tamaño 1, 2, 3, etc. Verifica que estas probabilidades suman 1. 
(supón que están definidos $p(a),p(b),p(</s>)$ y suman 1).

---

Para **bigramas** el proceso de generación es el siguiente:

1. Comenzamos con el símbolo $<s>$.  
2. Escogemos una palabra $w_1$ según las probabilidades ya definidas 

$$P(w^1|<s>), P(w^2|<s>), \ldots, P(w^N|<s>)$$ 

sobre el vocabulario. Estas probabilidades suman 1.

Comenzando con $i=1$,

3. Escogemos una palabra $w_{i+1}$ según las probabilidades ya definidas 

$$P(w^1|w_i), P(w^2|w_i), \ldots, P(w^N|w_i)$$ 

sobre el vocabulario. Estas probabilidades suman 1.
4. Si la palabra escogida es $</s>$ terminamos. Si no, repetimos 3 con $i=i+1$.


La probabilidad de una frase dada dado el modelo de bigramas es entonces:

$$P(<s>w_1\cdots w_n </s>) = P(w_1|<s>)P(w_2|w_1)P(w_3|w_2)  P(w_4|w_3) \cdots P(w_n|w_{n-1})P(</s>|w_n).$$


Por definición, la suma de probabilidades sobre todas las frases es 1. Necesitamos definir entonces
todas la probablidades
$$p(w|z),$$
donde $z$ es una palabra o $<s>$, y $w$ es una palabra o $</s>$.

Para **trigramas** podemos agregar un símbolo $<s><s>$ al inicio. Esto nos permite definir de manera
simple el proceso de generación como sigue:

1. Comenzamos con los dos símbolos $<s><s>$. 

Comenzando con $i=1$:

2. Cada palabra nueva $w_i$ se escoge según las probabilidades $P(w|w_1w_2)$ sobre $w$ en el vocabulario 
($w^1$, \ldots $w^N$). Estas probabilidades deben sumar uno. 
3. Terminamos cuando la palabra nueva escogida es $</s>$. Si no, repetimos 2 con $i=i+1$

Nótese que en la primera elección escogemos de $p(w|<s><s>)$ y luego de $p(w|<s> w^1)$.


La probabilidad de una frase dada dado el modelo de bigramas es entonces:

$$P(<s><s>w_1\cdots w_n </s>) = P(w_1|<s><s>)P(w_2|<s>w_1)P(w_3|w_1w_2)  P(w_4|w_2w_3) \cdots P(</s>|w_{n-1}w_n).$$



---

El modelo de unigramas
 puede ser deficiente para algunas aplicaciones,
y las probabilidades calculadas con este modelo pueden estar muy
lejos ser razonables. Por ejemplo, el modelo de unigramas da la
misma probabilidad a la frase *un día* y a la frase *día un*, aunque
la ocurrencia en el lenguaje de estas dos frases es muy distinta.

#### Ejemplo{-}
Supongamos que consideramos el modelo de bigramas,  y queremos calcular
la probabilidad de la frase "el perro corre". Agregamos inicio y final
de frase para obtener $<s> el perro corre </s> Y ahora pondríamos:
$$P(el,perro,corre) = P(el|<s>)P(perro \,|\, el)P(corre \,|\, perro)P(</s>|corre)$$ 
Nótese que aún para frases más largas, sólo es necesario definir
$P(w|z)$ para cada para de palabras del vocabulario $w,z$, lo cual
es un simplificación considerable.

---

La probabilidad de bigramas también se puede escribir como, usando las convenciones de 
caracteres de inicio y de final de frase, como

$$P(w_1\cdots w_n) = \prod_{j=1}^n P(w_j | w_{j -1}).$$


Y así podemos seguir. Por ejemplo, con el **modelo de trigramas**,

$$P(w_1\cdots w_n) = P(w_3|w_1w_2)  P(w_4|w_2w_3) \cdots P(w_n| w_{n-2} w_{n-1}).$$

En este caso, usamos dos símbolos de inicio de frase $<s> <s>$ para que la fórmula tenga sentido.

**Observación**: los modelos de n-gramas son aproximaciones útiles, y fallan en la modelación de dependencias de larga distancia. Por ejemplo, en la frase "Tengo un gato en mi casa, y es de tipo persa" tendríamos que considerar n-gramas imprácticamente largos para modelar correctamente la ocurrencia de la palabra "persa" al final de la oración.


## Modelo de n-gramas usando conteos

Supongamos que tenemos una colección de textos o frases del lenguaje
que nos interesa.
¿Cómo podemos estimar las probabilidades del tipo $P(w|a)$ $P(w|a,b)$?

El enfoque más simple es estimación por máxima verosimilitud, que 
en esto caso implica estimar con conteos de ocurrencia
en el lenguaje. Si queremos
estimar $P(w|z)$ (modelo de bigramas), entonces tomamos nuestra colección, y calculamos:

- $N(z,w)$ = número de veces que aparece $z$ seguida de $w$
- $N(z)$ = número de veces que aparece $z$.
- $P(w|z) = \frac{N(z,w)}{N(z)}$

Nótese que estas probabilidades en general son chicas (pues el
vocabulario es grande), de forma que conviene usar las log probabilidades
para hacer los cálculos y evitar *underflows*. Calculamos entonces usando:

- $\log{P(w|z)} = \log{N(zw)} - \log{N(z)}$

¿Cómo se estimarían las probabilidades para el modelo de unigramas
y trigramas?

#### Ejercicio {-}
Considera la colección de textos "un día muy soleado", 
"un día muy lluvioso", "un ejemplo muy simple de bigramas". Estima
las probabilidades $P(muy|<s>)$, $P(día | un)$, $P(simple | muy)$ usando
conteos.

#### Ejemplo {-}

Comenzamos por limpiar nuestra colección de texto, creando también  *tokens* adicionales para signos de puntuación. En este caso solo tenemos dos textos:

```{r}
normalizar <- function(texto, vocab = NULL){
  # minúsculas
  texto <- tolower(texto)
  # varios ajustes
  texto <- gsub("\\s+", " ", texto)
  texto <- gsub("\\.[^0-9]", " _punto_ ", texto)
  texto <- gsub(" _s_ $", "", texto)
  texto <- gsub("\\.", " _punto_ ", texto)
  texto <- gsub("[«»¡!¿?-]", "", texto) 
  texto <- gsub(";", " _punto_coma_ ", texto) 
  texto <- gsub("\\:", " _dos_puntos_ ", texto) 
  texto <- gsub("\\,[^0-9]", " _coma_ ", texto)
  texto <- gsub("\\s+", " ", texto)
  texto
}

corpus_mini <- c("Este es un ejemplo: el perro corre, el gato escapa. Este es un número 3.1416, otro número es 1,23.", "Este   es otro ejemplo.  " )
normalizar(corpus_mini)
```

Y ahora construimos, por ejemplo, los bigramas que ocurren en cada texto

```{r}
ejemplo <- tibble(txt = corpus_mini) %>%
                mutate(id = row_number()) %>%
                mutate(txt = normalizar(txt)) 
bigrams_ejemplo <- ejemplo %>% 
                   unnest_tokens(bigramas, txt, token = "ngrams", 
                                 n = 2) %>%
                   group_by(bigramas) %>% tally()
knitr::kable(bigrams_ejemplo)
```


## Notas de periódico: modelo de n-gramas simples.

En los siguientes ejemplos, utilizaremos una colección de
noticias cortas en español (de España).


```{r, message = FALSE, warning = FALSE}
library(tidyverse)
periodico <- read_lines(file= "../datos/noticias/Es_Newspapers.txt",
                        progress = FALSE)
length(periodico)
periodico[1:2]
```


```{r prepdf, cache = FALSE}
periodico_df <- tibble(txt = periodico) %>%
                mutate(id = row_number()) %>%
                mutate(txt = normalizar(txt)) 
```                

Adicionalmente, seleccionamos una muestra para hacer las
demostraciones (puedes correrlo con todo el corpus):

```{r}
set.seed(123)
muestra_ind <- sample(1:nrow(periodico_df), 1e5)
periodico_m <- periodico_df[muestra_ind, ]
```

Y calculamos las frecuencias de todos unigramas, bigramas y trigramas:

```{r unigramas, cache = FALSE}
conteo_ngramas <- function(corpus, n = 1, vocab_df = NULL){
  token_nom <- paste0('w_n_', rev(seq(1:n)) - 1)
  token_cond <- token_nom[-length(token_nom)]
  # añadir inicio de frases
  inicio <- paste(rep("_s_ ", n - 1), collapse = "")
  # añadir fin de frases
  fin <- " _ss_"
  ngramas_df <- corpus %>%
                mutate(txt = paste(inicio, txt, fin)) %>%
                unnest_tokens(ngrama, txt, token = "ngrams", n = n) 
  frec_ngramas <- ngramas_df %>% group_by(ngrama) %>%
                  summarise(num = length(ngrama)) %>%
                  separate(ngrama, token_nom, sep=" ") %>%
                  group_by(across(all_of(token_cond))) %>%
                  mutate(denom = sum(num)) %>%
                  ungroup %>%
                  mutate(log_p = log(num) - log(denom))
  frec_ngramas
}
mod_uni <- conteo_ngramas(periodico_m, n = 1)
mod_bi  <- conteo_ngramas(periodico_m, n = 2)
mod_tri <- conteo_ngramas(periodico_m, n = 3)
```

```{r}
mod_uni %>% arrange(desc(num)) %>% head(100) %>% knitr::kable()
```



```{r}
mod_bi %>% arrange(desc(num)) %>% head(100) %>% knitr::kable()
```

¿Qué palabra es más probable que aparezca después de *en*,
la palabra *la* o la palabra *el*?

```{r}
mod_tri %>% arrange(desc(num)) %>% head(100) %>% knitr::kable()
```

## Problema de los ceros {-}

Podemos ahora evaluar la probabilidad de ocurrencia de 
textos utilizando las frecuencias que calculamos arriba:

```{r}
n_gramas <- list(unigramas = mod_uni,
                 bigramas  = mod_bi,
                 trigramas = mod_tri)

log_prob <- function(textos, n_gramas, n = 2, laplace = FALSE, delta = 0.001, vocab_env = NULL){
  df <- tibble(id = 1:length(textos), txt = textos) %>%
         mutate(txt = normalizar(txt)) 
  if(!is.null(vocab_env)){
    df <- df %>% mutate(txt_u = map_chr(txt, ~restringir_vocab(.x, vocab = vocab_env))) %>% 
    select(id, txt_u) %>% rename(txt = txt_u)
  }
  token_nom <- paste0('w_n_', rev(seq(1:n)) - 1)
  df_tokens <- df %>% group_by(id) %>%
                unnest_tokens(ngrama, txt, 
                token = "ngrams", n = n) %>%
                separate(ngrama, token_nom, " ") %>%
                left_join(n_gramas[[n]], by = token_nom)
  if(laplace){
    V <- nrow(n_gramas[[1]])
    log_probs <- log(df_tokens[["num"]] + delta) - log(df_tokens[["denom"]] + delta*V )
    log_probs[is.na(log_probs)] <- log(1/V)
  } else {
    log_probs <- df_tokens[["log_p"]]
  }
  log_probs <- split(log_probs, df_tokens$id)
  sapply(log_probs, mean)
}
```

```{r}
textos <- c("un día muy soleado",
            "este de es ejemplo un",
            "este es un ejemplo de",
            "esta frase es exotiquísima")
log_prob(textos, n_gramas, n = 1)
log_prob(textos, n_gramas, n = 2)
log_prob(textos, n_gramas, n = 3)
```


**Observaciones**:

- El modelo de unigramas claramente no captura estructura en el orden de palabras. La segunda frase por ejemplo,
tiene probabilidad alta porque tiene tokens o palabras comunes, pero la frase en realidad tendría probabilidad muy
baja de ocurrir en el lenguaje. Esto parece incorrecto.
- La cuarta frase tiene probabilidad 0 en todos los modelos porque la palabra *exotiquísima* no existe en el vocabulario de entrenamiento. Esto parece incorrecto.
- La primera frase tiene probabilidad 0 en el modelo de bigramas y trigramas, porque nunca encontró alguna serie de tres palabras juntas. Esto parece incorrecto.

Incluso el modelo de bigramas puede dar probabilidad cero a frase que deberían ser relativamente comunes,
pues el conjunto de frases es muy grande:

```{r}
n <- 2
textos <- "Otro día muy soleado"
df <- tibble(id = 1:length(textos), txt = textos) %>%
         mutate(txt = normalizar(txt))
token_nom <- paste0('w_n_', rev(seq(1:n)) - 1)
df_tokens <- df %>% group_by(id) %>%
                unnest_tokens(ngrama, txt, 
                token = "ngrams", n = n) %>%
                separate(ngrama, token_nom, " ") %>%
                left_join(n_gramas[[n]], by = token_nom)
df_tokens
```

El problema es que no observamos "muy soleado", y esta frase tendría 0 probabilidad de ocurrir.

---

Esta última observación es importante: cuando no encontramos en nuestros conteos
un bigrama (o trigrama, etc.) dado, la probabilidad asignada es 0. 

Aunque para algunas frases esta asignación es correcta (una sucesión de palabras que casi no puede
ocurrir en el lenguaje), muchas veces esto se debe a que los datos son ralos: la mayor
parte de las frases posibles no son observadas en nuestro corpus, y es erróneo
asignarles probabilidad 0. Más en general, cuando los conteos de bigramas son
chicos, sabemos que nuestra estimación por máxima verosimilitud tendrá varianza 
alta.

Estos ceros ocurren de dos maneras:

- Algunas palabras son *nuevas*: no las observamos en nuestros
datos de entrenamiento.
- No observamos bigramas, trigramas, etc. específicos (por ejemplo,
observamos *día*, y *aburrido* pero no observamos *día aburrido*).

#### Palabras desconocidas {-}

Para el primer problema, podemos entrenar nuestro modelo
con una palabra adicional $<unk>$, que denota palabras desconocidas. Una
estrategia es tomar las palabras con frecuencia baja y sustituirlas por
<unk>, por ejemplo:

```{r}
vocabulario_txt <- n_gramas[[1]] %>% filter(num > 1) %>% 
    pull(w_n_0)
vocab_env <- new.env()
vocab_env[["_unk_"]] <- 1
for(a in vocabulario_txt){
    vocab_env[[a]] <- 1
}
nrow(n_gramas[[1]])
sum(n_gramas[[1]]$num)
length(vocab_env)
```

```{r}
restringir_vocab <- function(texto, vocab_env){
  texto_v <- strsplit(texto, " ")[[1]]
  texto_v <- lapply(texto_v, function(x){
    if(x != ""){
        en_vocab <- vocab_env[[x]]
        if(is.null(en_vocab)){
            x <- "_unk_"
        }
        x
    }
  })
  texto <- paste(texto_v, collapse = " ")
  texto
}
periodico_m_unk <- periodico_m %>% 
    mutate(txt_u = map_chr(txt, ~restringir_vocab(.x, vocab_env = vocab_env))) %>% 
    select(id, txt_u) %>% rename(txt = txt_u)
```


Y ahora podemos reentrenar nuestros modelos:

```{r, cache=FALSE}
mod_uni <- conteo_ngramas(periodico_m_unk, n = 1)
mod_bi  <- conteo_ngramas(periodico_m_unk, n = 2)
mod_tri <- conteo_ngramas(periodico_m_unk, n = 3)
n_gramas_u <- list(mod_uni, mod_bi, mod_tri)
```

```{r}
textos <- c("un día muy soleado",
            "este de es ejemplo un",
            "este es un ejemplo de",
            "esta frase es exotiquísima")
log_prob(textos, n_gramas_u, n = 1, vocab_env = vocab_env)
log_prob(textos, n_gramas_u, n = 2, vocab_env = vocab_env)
log_prob(textos, n_gramas_u, n = 3, vocab_env = vocab_env)
```

Y con esto podemos resolver el problema de vocabulario desconocido.

#### Contexto no observado {-}

Para el segundo problema, existen **técnicas de suavizamiento** (que veremos más adelante). El método más simple es el **suavizamiento de Laplace**,
en el que simplemente agregamos una cantidad $\delta$ a los conteos de unigramas, bigramas, etc.

Para unigramas, agregamos $\delta$ a cada posible unigrama. Si el tamaño del vocabulario
es $V$, y el conteo total de *tokens* es $N$, el nuevo conteo de *tokens* será entonces
$N+\delta V$.  Por ejemplo, la probabilidad para bigramas es:

$$P(w|a) = \frac{N(aw)}{N(a)} = \frac{N(aw)}{\sum_{z\in V} N(az)},$$
De modo que la estimación suavizada (sumando $\delta$ a cada bigrama) es
$$P_{L}(w|a) = \frac{N(aw)+\delta}{\sum_{z\in V} N(az)+\delta}= \frac{N(aw) + \delta}{N(z)+ \delta V} $$
Para trigramas,
$$P_{L}(w|ab) = \frac{N(abw)+\delta}{\sum_{z\in V} N(abz)+\delta}= \frac{N(abw) + \delta}{N(ab)+ \delta V} $$
y así sucesivamente.

**Observación**: este método es útil para introducir la idea de
suavizamiento, pero existen otros mejores que veremos más adelante
(ver sección 4.5 de [@jurafsky], o 3.5 en la edición más reciente). Veremos más adelante también cómo
escoger hiperparámetros como $\delta$.

```{r}
log_prob(textos, n_gramas_u, n = 1, laplace = TRUE, delta = 0.01)
log_prob(textos, n_gramas_u, n = 2, laplace = TRUE, delta = 0.01)
log_prob(textos, n_gramas_u, n = 3, laplace = TRUE, delta = 0.01)
```

Nótese que este suavizamiento cambia considerablemente las probabilides estimadas,
incluyendo algunas de ellas con conteos altos (esta técnica dispersa demasiada
probabilidad sobre conteos bajos).


## Evaluación de modelos

### Generación de texto

Una primera idea de qué tan bien funcionan estos modelos es generando
frases según las probabilidades que estimamos.

#### Ejemplo: unigramas {-}

Generamos algunas frases bajo el modelo de unigramas. Podemos construir
una frase comenzando con el token $<s>$, y paramos cuando encontramos
$</s>$. Cada token se escoge al azar según las probablidades $P(w)$.

```{r}
calc_siguiente_uni <- function(texto, n_gramas){
  u <- runif(1)
  unigramas_s <- arrange(n_gramas[[1]], log_p) 
  prob_acum <- cumsum(exp(unigramas_s$log_p))
  palabra_no <- match(TRUE, u < prob_acum)
  as.character(unigramas_s[palabra_no, "w_n_0"])
}
texto <- ""
fin <- FALSE
set.seed(1215)
while(!fin){
  siguiente <- calc_siguiente_uni(texto, n_gramas)
  texto <- c(texto, siguiente)
  if(siguiente == "_ss_"){
    fin <- TRUE
  }
}
paste(texto, collapse = " ")
```


#### Ejemplo: bigramas

```{r}
calc_siguiente_bi <- function(texto, n_gramas){
  u <- runif(1)
  n <- length(texto)
  anterior <- texto[n]
  siguiente_df <- filter(n_gramas[[2]], w_n_1 == anterior) %>% 
    arrange(log_p) 
  palabra_no <- match(TRUE, u < cumsum(exp(siguiente_df$log_p)))
  as.character(siguiente_df[palabra_no, "w_n_0"])
}
texto <- "_s_"
set.seed(4123)
fin <- FALSE
while(!fin){
  siguiente <- calc_siguiente_bi(texto, n_gramas)
  texto <- c(texto, siguiente)
  if(siguiente == "_ss_"){
    fin <- TRUE
  }
}
paste(texto, collapse = " ")
```


#### Ejemplo: trigramas

```{r}
calc_siguiente_tri <- function(texto, n_gramas){
  u <- runif(1)
  n <- length(texto)
  contexto <- texto[c(n,n-1)]
  siguiente_df <- filter(n_gramas[[3]], w_n_1 == contexto[1], w_n_2 == contexto[2]) %>% 
    arrange(log_p)
  palabra_no <- match(TRUE, u < cumsum(exp(siguiente_df$log_p)))
  as.character(siguiente_df[palabra_no, "w_n_0"])
}
texto <- c("_s_","_s_")
set.seed(4122)
fin <- FALSE
while(!fin){
  siguiente <- calc_siguiente_tri(texto, n_gramas)
  texto <- c(texto, siguiente)
  if(siguiente == "_ss_"){
    fin <- TRUE
  }
}
paste(texto, collapse = " ")
```

**Observación**: en este ejemplo vemos cómo los textos parecen más textos reales
cuando usamos n-gramas más largos. 

### Evaluación de modelos: perplejidad

 En general,
la mejor manera de hacer la evaluación de un modelo de lenguaje es en el contexto
de su aplicación (es decir, el desempeño en la tarea final para el que construimos
nuestro modelo): por ejemplo, si se trata de corrección de ortografía, qué tanto da la palabra
correcta, o qué tanto seleccionan usuarios palabras del corrector.

Sin embargo también podemos hacer una evaluación intrínseca del modelo considerando
muestras de entrenamiento y prueba. Una medida usual en este contexto
es la **perplejidad**.

```{block2, type='resumen'}
Sea $P(W)$ un modelo del lenguaje. Supongamos que observamos
un texto $W=w_1 w_2\cdots w_N$ (una cadena larga, que puede incluír varios separadores
$</s>, <s>$). La **log-perplejidad** del modelo sobre este texto es igual a
$$LP(W) = -\frac{1}{N} \log P(w_1 w_2 \cdots w_N),$$
  que también puede escribirse como
$$LP(W) = -\frac{1}{N} \sum_{i=1}^N \log P(w_i|w_1 w_2 \cdots w_{i-1})$$
La perplejidad es igual a
$$PP(W) = e^{LP(W)},$$ 
que es la medida más frecuentemente reportada en modelos de lenguaje.
```

**Observaciones**: 

- La log perplejidad es similar a la devianza (negativo de log-verosimilitud)
de los datos (palabras) observadas $W$ bajo el modelo $P$.
- Cuanto más grande es $P(W)$ bajo el modelo, menor es la perplejidad.
- Mejores modelos tienen valores más bajos de perplejidad. Buscamos
modelos que asignen muy baja probabilidad a frases que no son 
gramáticas, o no tienen sentido, y alta probabilidad a frases
que ocurren con frecuencia.

Bajo el modelo de bigramas, por ejemplo, tenemos que

$$LP(W) = -\frac{1}{N} \sum_{i=1}^N \log P(w_i|w_{i-1})$$

Y para el modelo de trigramas:
$$LP(W) = -\frac{1}{N} \sum_{i=1}^N \log P(w_i|w_{i-2}w_{i-1})$$

```{block2, type='resumen'}
- La evaluación de modelos la hacemos calculando la perplejidad en una
muestra de textos de prueba, que no fueron utilizados para
entrenar los modelos.
- Para palabras no vistas, entrenamos nuestros modelos sustituyendo
palabras no frecuentes con $<unk>$, y a las palabras no vistas
les asignamos el token $<unk>$.
```

---

#### Ejemplo {-}

Podemos usar nuestra función anterior para calcular la perplejidad. Para
los datos de entrenamiento vemos que la perplejidad de entrenamiento es mejor para
los modelos más complejos:

```{r}
periodico_entrena <- periodico[muestra_ind]
textos <- periodico_entrena[1:1000]
texto_entrena <- paste(textos, collapse = " ")
exp(-log_prob(texto_entrena, n_gramas_u, n = 1, laplace = T))
exp(-log_prob(texto_entrena, n_gramas_u, n = 2, laplace = T))
exp(-log_prob(texto_entrena, n_gramas_u, n = 3, laplace = T))
```

Pero para la muestra de prueba:



```{r}
periodico_prueba <- periodico[-muestra_ind]
textos <- periodico_prueba[1:1000]
texto_prueba <- paste(textos, collapse = " ")
exp(-log_prob(texto_prueba, n_gramas_u, n = 1, laplace = T))
exp(-log_prob(texto_prueba, n_gramas_u, n = 2, laplace = T))
exp(-log_prob(texto_prueba, n_gramas_u, n = 3, laplace = T))
```

Y vemos para este ejemplo que el mejor desempeño está dado por el modelo de bigramas, aunque no hemos hecho ningún ajuste ad-hoc del parámetro $\delta$. Como explicamos arriba,
es preferible usar otros algoritmos de suavizamiento.


####  Ejercicio {-}
Discute por qué: 

- El modelo de bigramas se desempeña mejor que el de unigramas
en la muestra de prueba.
- Parece que el modelo de trigramas, con 
nuestra colección de textos chicos, parece "sobreajustar", y tener
mal desempeño sobre la muestra de prueba. 


## Suavizamiento de conteos: otros métodos

Como discutimos arriba, el primer problema para generalizar
a frases o textos que no hemos visto, es que muchas veces no
observamos en nuestros datos de entrenamiento 
ciertos n-gramas con probabiliidad relativamente alta. Arriba
propusimos el suavizamiento de Laplace, que es una manera
burda de evitar los ceros y se basa en la idea de quitar
un poco de probabilidad a los n-gramas observados y redistribuir
sobre los no observados.

Podemos considerar métodos mejores observando que cuando
el contexto es muy grande (por ejemplo, trigramas), es fácil
no encontrar 0's en frases usuales del lenguaje (por ejemplo,
encontramos "Una geometría euclideana", "Una geometría no-euclideana",
pero nunca observamos Una geometría hiperbólica"). En este
caso, podríamos bajar a usar los *bigramas*, y considerar solamente
el bigrama "geometría hiperbólica", que quizá tiene algunas
ocurrencias en la muestra de entrenamiento. A este proceso se 
le llama **backoff**.

Un método popular con mejor desempeño que Laplace es
el método de descuento absoluto (da) de **Kneser-Ney**. Consideremos el caso de bigramas.

En primer lugar, este método utiliza interpolación entre bigramas y unigramas, 
considerando que debemos quitar algo de masa de probabilidad 
a bigramas observados para distribuir en bigramas no observados. El descuento lo
hacemos absoluto (por ejemplo restando $d=0.75$, ver [@jurafsky])

$$P_{da}(w|a) = \frac{N(aw) - d}{N(a)} + \lambda(a) P(w).$$

Así que reducimos los conteos observados por una fracción
$d$, e interpolamos con las probabilidad de ocurrencia de unigramas. $\lambda(a)$ es tal que la suma de todas las probabilidades es igual a 1: $\sum_w P(w|a) = 1$. El lado derecho de esta ecuación toma valores positivos
siempre y cuando $N(w)\geq 1$, que suponemos (vocabulario completo, quizá incluyendo unk).

Esta primera parte de descuento absoluto pone masa de probabilidad sobre bigramas no vistos. Podemos
mejorar si en lugar de considerar $P(w)$ consideramos una cantidad más específica, como sigue:

En vez de usar las probabilidades crudas
de unigramas $P(w)$, usamos las probabilidades de continuación
de unigramas $P_{cont}(w)$, pues aquí $w$ aparece como continuación después de $a$:

$$P_{cont}(w) = \frac{\sum_{z\in V} I(N(zw) >0) }
{ \sum_{z, b\in V} I(N(zb > 0))}.$$

Que simplemente es la probabilidad de que $w$ sea una continuación después de alguna palabra (en el denominador están todas las posibles continuaciones sobre todas las palabras). Verifica que $\sum_{w\in V} P_{cont} (w) = 1$, de forma que es una distribución de probabilidad
sobre los unigramas.

La idea de este método se basa en la siguiente observación: 

- Hay palabras como *embargo* que principalmente ocurren como continuación
de palabras específicas (**sin embargo**). En general, su probabilidad de continuación es baja. 
- Pero *embargo* puede tener probabilidad de unigrama alto porque *sin embargo* ocurre frecuentemente.
- En una frase como "Este es un día ...", quizá *aburrido* ocurre menos que *embargo*, pero *aburrido* es una continuación más apropiada, pues ocurre más como continuación de otras palabras.
- Si usamos las probabilidad de continuación, veríamos que 
$P_{cont}(embargo)$ es baja, pero $P_{cont}(aburrido)$ es más alta,
pues *aburrido* ocurre más como continuación.

Usaríamos entonces:

$$P_{da}(w|a) = \frac{N(aw) - d}{N(a)} + \lambda(a) P_{cont}(w).$$


### Desempeño {-}

Cuando construimos modelos grandes, una búsqueda directa en
una base de datos usual de n-gramas puede ser lenta (por ejemplo,
considera cómo funcionaría el *auto-complete* o la corrección de ortografía).

Hay varias alternativas. Se pueden filtrar los n-gramas menos frecuentes (usando las ideas de arriba para hacer backoff o interpolación con unigramas), y utilizar estructuras
apropiadas para encontrar los n-gramas. Otra solución es utilizar algoritmos más simples como
**stupid backoff** (que se usa para colecciones muy grandes de texto),
que consiste en usar la probabilidad del (n-1)-grama multiplicado
por una constante fija $\lambda$ si no encontramos el n-grama
que buscamos (Ver [@jurafsky]).

Finalmente, una opción es utilizar **filtros de bloom** para
obtener aproximaciones de los conteos. Si la frecuencia
de un n-grama $w$ es $f$, por ejemplo, insertamos en 
el filtro el elemento $(w, 2^j)$ para toda $j$ tal que
$2^j < f$. Para estimar un conteo de un n-grama observado,
simplemente checamos sucesivamente si el elemento $(w, 2^j)$ está
en el filtro o no, hasta que encontramos una $k$ tal que
$(w, 2^{k+1})$ no está en el filtro. Nuestra estimación
de la frecuencia del n-grama $w$ es entonces $2^k \leq f < 2^{k+1}$


## Ejemplo: Corrector de ortografía

Como ejemplo del canal ruidoso y el papel que juega los modelos del lenguaje, podemos
ver un sistema simple de corrección de ortografía.

En nuestro caso, usaremos como $P(W)$ el modelo de bigramas, y el modelo de ruido
$P(X|W)$ es simple: lo construimos a partir de palabras $p(x|w)$, y consideramos que dada
una palabra $w$, es posible observar $x$, donde $w$ se produce como transformación de $x$ aplicando:

1. Una eliminación de un caracter
2. Inserción de un caracter
2. Sustitución de un caracter
3. Transposición de dos caracteres adyacentes

Daremos igual probabilidad a todas estas posiblidades, aunque en la realidad este modelo
debería ser más refinado (¿cómo mejorarías este modelo?¿Con qué datos?).

En el siguiente paso tendríamos que producir sugerencias de corrección.
En caso de encontrar una palabra que no está en el diccionario,
podemos producir palabras similares (a cierta distancia de edición),
y filtrar aquellas que están en el vocabulario (ver [How to write a spelling corrector](http://norvig.com/spell-correct.html)).



```{r}
generar_candidatos <- function(palabra){
  caracteres <- c(letters, 'á', 'é', 'í', 'ó', 'ú', 'ñ')
  pares <- lapply(0:(nchar(palabra)), function(i){
    c(str_sub(palabra, 1, i), str_sub(palabra, i+1, nchar(palabra)))
  })
  eliminaciones <- pares %>% map(function(x){ paste0(x[1], str_sub(x[2],2,-1))})
  sustituciones <- pares %>% map(function(x)
      map(caracteres, function(car){
    paste0(x[1], car, str_sub(x[2], 2 ,-1))
  })) %>% flatten 
  inserciones <- pares %>% map(function(x){
    map(caracteres, function(car) paste0(x[1], car, x[2]))
  }) %>% flatten
  transposiciones <- pares %>% map(function(x){
    paste0(x[1], str_sub(x[2],2,2), str_sub(x[2],1,1), str_sub(x[2],3,-1))
  })
  c(eliminaciones, sustituciones, transposiciones, inserciones)
}
```

```{r}
candidatos <- generar_candidatos("solado")
sprintf("Número de candidatos generados: %0.i", length(candidatos))
print("Algunos candidatos:")
head(candidatos)
```

```{r}
sugerir <- function(frase, mod_bi){
  tokens <- normalizar(frase) %>% str_split(" ") %>% first %>% tail(2)
  candidatos <- generar_candidatos(tokens[2])
  candidatos_tbl <- mod_bi %>% filter(w_n_1 == tokens[1], w_n_0 %in% c(tokens[2], candidatos)) %>% 
    mutate(log_posterior = 0 + log_p) # suponemos todos las corrupciones igualmente probabiles
  arrange(candidatos_tbl, desc(log_posterior))
}
```

```{r}
sugerir(c("esto es un echo"), mod_bi) %>% head()
```

```{r}
sugerir("dice que lo hechó", mod_bi)
```

```{r}
sugerir("un pantalón asul", mod_bi)
```

Este modelo está basado en bigramas, pero podemos usar también unigramas por ejemplo para
producir recomendaciones cuando no encontremos bigramas apropiados.



