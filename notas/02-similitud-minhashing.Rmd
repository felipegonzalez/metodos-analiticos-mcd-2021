# Similitud y minhashing

En la primera parte del curso tratamos un problema fundamental en varias tareas de análisis de datos: 

- ¿Cómo medir similitud entre objetos o casos?
- ¿Cómo encontrar vecinos cercanos en un conjunto de datos?
- ¿Cómo hacer uniones de tablas por similitud?

Algunos ejemplos son:

- Encontrar documentos similares en una colección de documentos. Esto puede 
servir para detectar
plagio, deduplicar noticias o páginas web, hacer *matching* de datos
de dos fuentes (por ejemplo, nombres completos de personas),
etc. Ver por ejemplo [Google News]((https://dl.acm.org/citation.cfm?id=1242610)).
- Encontrar usuarios similares (Netflix), en el sentido de que tienen gustos similares, o películas similares, en el sentido de qe le gustan a las mismas personas.
- Encontrar imágenes similares en una colección grande, ver por ejemplo [Pinterest](https://medium.com/@Pinterest_Engineering/detecting-image-similarity-using-spark-lsh-and-tensorflow-618636afc939).
- Uber: rutas similares que indican (fraude o abusos)[https://eng.uber.com/lsh/].
- Deduplicar registros de usuarios de algún servicio (por ejemplo, beneficiarios
de programas sociales).

Estos problemas no son triviales por dos razones:

- Los elementos que queremos comparar muchas veces están naturalmente representados en espacios de dimensión alta, y es relativamente costoso comparar un par (documentos, imágenes, usuarios, rutas). Muchas veces es preferible construir una representación más compacta y hacer comparaciones con las versiones comprimidas.
- Si la colección de elementos es grande ($N$), entonces el número de pares 
posibles es del orden de $N^2$, y no es posible hacer todas las posibles comparaciones para encontrar los elementos similares (por ejemplo, comparar
$100$ mil documentos, con unas $10$ mil comparaciones por segundo, tardaría alrededor de $5$ días).

Si tenemos que calcular *todas* las similitudes, no hay mucho qué hacer. Pero
muchas veces nos interesa encontrar pares de similitud alta, o completar tareas
más específicas como contar duplicados, etc. En estos casos, veremos que es
posible construir soluciones probabilísticas aproximadas para resolver estos
problemas de forma escalable. 

Aunque veremos más adelante métricas de similitud comunes como
la dada por la distancia euclideana o distancia coseno, por ejemplo, en 
esta primera parte nos concentramos en discutir similitud entre
pares de textos. Los textos los podemos ver como colecciones de palabras, o
de manera más general, como colecciones de cadenas.


## Similitud de conjuntos

Muchos de estos problemas de similitud se pueden pensar como 
problemas de similitud entre conjuntos. Por ejemplo, los documentos son conjuntos de palabras, conjuntos
de pares de palabras, sucesiones de caracteres,
una película se puede ver como el conjunto de personas a las que le gustó, o una ruta
como un conjunto de tramos, etc.

Hay muchas medidas que son útiles para cuantificar la similitud entre conjuntos. Una que es popular, y que explotaremos por sus propiedades, es la similitud de Jaccard:


```{block2, type='resumen'}
La **similitud de Jaccard** de los conjuntos $A$ y $B$ está dada por

$$sim(A,B) = \frac{|A\cap B|}{|A\cup B|}$$

```

Esta medida cuantifica qué tan cerca está la unión de $A$ y $B$ de su intersección. Cuanto más parecidos sean $A\cup B$ y $A\cap B$, más similares son los conjuntos. En términos geométricos, es el área de la intersección entre el área de la unión. 

#### Ejercicio {-}

Calcula la similitud de Jaccard entre los conjuntos $A=\{5,2,34,1,20,3,4\}$
 y $B=\{19,1,2,5\}$
 

```{r, collapse = TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
options(digits = 3)

sim_jaccard <- function(a, b){
    length(intersect(a, b)) / length(union(a, b))
}

sim_jaccard(c(0,1,2,5,8), c(1,2,5,8,9))
sim_jaccard(c(2,3,5,8,10), c(1,8,9,10))
sim_jaccard(c(3,2,5), c(8,9,1,10))
```


## Representación de documentos como conjuntos

Hay varias maneras de representar documentos como conjuntos. Las más simples son:

1. Los documentos son colecciones de palabras, o conjuntos de sucesiones de palabras de tamaño $n$.
2. Los documentos son colecciones de caracteres, o conjuntos de sucesiones de caracteres (cadenas) de tamaño $k$.


La primera representación se llama *representación de n-gramas*, y la segunda *representación de k-tejas*, 
o $k$-_shingles_. Nótese que en ambos casos, representaciones de dos documentos con secciones parecidas acomodadas en distintos lugares tienden a ser similares.

Consideremos una colección de textos cortos:

```{r}
textos <- character(4)
textos <- c("el perro persigue al gato, pero no lo alcanza", 
            "el gato persigue al perro, pero no lo alcanza", 
            "este es el documento de ejemplo", 
            "el documento habla de perros, gatos, y otros animales")
```

Abajo mostramos la representacion en bolsa de palabras (1-gramas) y la representación en bigramas (2-gramas) de los primeros dos documentos:

```{r}
# Bolsa de palabras (1-gramas)
tokenizers::tokenize_ngrams(textos[1:2], n = 1) %>% map(unique)
```

```{r}
# bigramas
tokenizers::tokenize_ngrams(textos[1:2], n = 2) %>% map(unique)
```

La representación en _k-tejas_ es otra posibilidad:

```{r}
calcular_tejas <- function(x, k = 2){
  tokenizers::tokenize_character_shingles(x, n = k, lowercase = FALSE,
    simplify = TRUE, strip_non_alpha = FALSE)
}
# 2-tejas
calcular_tejas(textos[1:2], k = 2) %>% map(unique)
# 4-tejas:"
calcular_tejas(textos[1:2], k = 4) %>% map(unique)
```


**Observaciones**:

1. Los _tokens_ son las unidades básicas de análisis. Los _tokens_ son palabras para los n-gramas (cuya definición no es del todo simple) y caracteres para las k-tejas. Podrían ser también oraciones, por ejemplo.
2. Nótese que en ambos casos es posible hacer algo de preprocesamiento para
obtener la representación. Transformaciones usuales son:

  - Eliminar puntuación y/o espacios. 
  - Convertir los textos a minúsculas.
  - Esto incluye decisiones acerca de qué hacer con palabras compuestas (por ejemplo, con un guión), palabras que denotan un concepto (Reino Unido, por ejemplo) y otros detalles.

3. Si lo que nos interesa principalmente
similitud textual (no significado, o polaridad, etc.) entre documentos, entonces podemos usar $k$-tejas, con un mínimo de preprocesamiento. Esta
representación es **simple y flexible** en el sentido de que se puede adaptar para documentos muy cortos (mensajes o tweets, por ejemplo), pero también para documentos más grandes.

Por estas razones, no concentramos por el momento en $k$-tejas


```{block2, type = 'resumen'}
**Tejas (shingles)**
  
Sea $k>0$ un entero. Las $k$-tejas ($k$-shingles) de un documento d
 es el conjunto de todas las corridas (distintas) de $k$
caracteres sucesivos.

Escogemos $k$ suficientemente grande, de forma que la probabilidad de que
una teja particular ocurra en un texto dado sea relativamente baja.
```


#### Ejemplo {-}
Documentos textualmente similares tienen tejas similares:

```{r, collapse = TRUE}
# calcular tejas
textos
tejas_doc <- calcular_tejas(textos, k = 4)
# calcular similitud de jaccard entre algunos pares
sim_jaccard(tejas_doc[[1]], tejas_doc[[2]])
sim_jaccard(tejas_doc[[1]], tejas_doc[[3]])
sim_jaccard(tejas_doc[[4]], tejas_doc[[3]])
```

Podemos calcular todas las similitudes:

```{r}
tejas_tbl <- crossing(id_1 = 1:length(textos), id_2 = 1:length(textos)) %>%
  filter(id_1 < id_2) %>% 
  mutate(tejas_1 = tejas_doc[id_1], tejas_2 = tejas_doc[id_2])
tejas_tbl
```

```{r}
tejas_tbl %>% 
  mutate(sim = map2_dbl(tejas_1, tejas_2, ~sim_jaccard(.x, .y))) %>% 
  select(id_1, id_2, sim)
```

pero nótese que, como señalamos arriba, esta operación será muy
costosa incluso si la colección de textos es de tamaño moderado.


- Si los textos
son cortos, entonces basta tomar valores como $k=4,5$, pues hay un total de $27^4$ tejas
de tamaño $4$, y el número de tejas de un documento corto (mensajes, tweets) es mucho más bajo que
$27^4$ (nota: ¿puedes explicar por qué este argumento no es exactamente correcto?)

- Para documentos grandes, como noticias o artículos, es mejor escoger un tamaño más grande,
como $k=9,10$, pues en documentos largos puede haber cientos de miles
de caracteres. Si $k$ fuera más chica entonces una gran parte de las tejas aparecerá en muchos de los documentos, y todos los documentos tendrían similitud alta.

- Evitamos escoger $k$ demasiado grande, pues entonces los únicos documentos similares tendrían
que tener subcadenas largas exactamente iguales. Por ejemplo: "Batman y Robin" y "Robin y Batman" son algo
similares si usamos tejas de tamaño 4, pero son muy distintas si usamos tejas de tamaño 8:

#### Ejemplo {-}

```{r}
tejas_1 <- calcular_tejas("Batman y Robin", k = 4)
tejas_2 <- calcular_tejas("Robin y Batman", k = 4)
sim_jaccard(tejas_1, tejas_2)
tejas_1 <- calcular_tejas("Batman y Robin", k = 8)
tejas_2 <- calcular_tejas("Robin y Batman", k = 8)
sim_jaccard(tejas_1, tejas_2)
```



## Representación matricial

Podemos usar una matriz binaria para guardar todas las
representaciones en k-tejas de nuestra colección de documentos. Puede usarse
una representación rala (_sparse_) si es necesario:

```{r}
dtejas_tbl <- tibble(id = paste0("doc_", 1:length(textos)), 
    tejas = tejas_doc) %>% 
  unnest(cols = tejas) %>% 
  unique %>% mutate(val = 1) %>% 
  pivot_wider(names_from = id, values_from = val, values_fill = list(val = 0)) %>% 
  arrange(tejas) # opcionalmente ordenamos tejas
dtejas_tbl
```


¿Cómo calculamos la similitud de Jaccard usando estos datos?

Calcular la unión e intersección se puede hacer haciendo OR y AND de las columnas, y
entonces podemos calcular la similitud
```{r}
inter_12 <- sum(dtejas_tbl$doc_1 & dtejas_tbl$doc_2)
union_12 <- sum(dtejas_tbl$doc_1 | dtejas_tbl$doc_2)
similitud <- inter_12/union_12
similitud # comparar con el número que obtuvimos arriba.
```

El cálculo para todos los documentos podríamos hacerlo (aunque veremos que normalmente
no haremos esto si no necesitamos calcular todas las similitudes) con:

```{r}
mat_td <- t(dtejas_tbl %>% select(-tejas) %>% as.matrix)
1 - dist(mat_td, method = "binary")
```




## Minhash y reducción probabilística de dimensionalidad

Para una colección grande de documentos
la representación binaria de la colección de documentos 
puede tener un número muy grande de renglones. Puede ser posible
crear un número más chico de nuevos _features_ (ojo: aquí los renglones
son las "variables", y los casos son las columnas) con los que
sea posible obtener una buena aproximación de la similitud.

Los mapeos que usaremos son escogidos al azar, y son sobre
el espacio de enteros.

- Sea $\pi$ una permutación al azar de los renglones de la matriz.
- Permutamos los renglones de la matriz tejas-documentos según $\pi$.
- Definimos una nuevo descriptor, el **minhash** del documento: para cada documento (columna) $d$ de la matriz permutada, tomamos el entero $f_\pi (d)$ que da el 
número del primer renglón que es distinto de $0$.


#### Ejercicio {#ej1}

Considera la matriz de tejas-documentos para cuatro documentos y cinco tejas
dada a continuación, con las permutaciones $(2,3,4,5,1)$ (indica que el renglón
$1$ va al $2$, el $2$ a $3$, el $5$ al $1$, etc.) y $(2,5,3,1,4)$. Calcula el descriptor definido arriba.

```{r, echo = FALSE}
mat <- matrix(c(c(1,0,0,1), c(0,0,1,0), 
            c(0,1,0,1), c(1,0,1,1),
            c(0,0,1,0)), nrow = 5, ncol = 4, byrow = TRUE)
colnames(mat) <- c('d_1','d_2','d_3','d_4')
rownames(mat) <- c('abc', 'ab ','xyz','abx','abd')
mat
```

```{r, echo=FALSE, include=FALSE}
perm_1 <- c(2,3,4,5,1)
perm_2 <- c(2,5,3,1,4)
apply(mat[order(perm_1), ]==1, 2, which) %>% map(min)
apply(mat[order(perm_2), ]==1, 2, which) %>% map(min)
```



#### Ejemplo {-}

Ahora regresamos a nuestro ejemplo de $4$ textos chicos.
Por ejemplo, para una permutación tomada al azar:

```{r}
set.seed(321)
df_1 <- dtejas_tbl %>% sample_n(nrow(dtejas_tbl))
df_1
```

Los minhashes para cada documentos con estas permutaciones son:

```{r}
df_1 %>% summarise_at(vars(matches('doc')), detect_index, ~.x == 1) 
```

Ahora repetimos con otras permutaciones:

```{r}
calc_firmas_perm <- function(df, permutaciones){
    salida <- map(permutaciones, function(perm){
        # permutar matriz (quisiéramos evitar)
        df_1 <- df[order(perm), ]
        # encontrar el primer uno
        firma <- df_1 %>% summarise_at(vars(matches('doc')), detect_index, ~.x == 1)
        firma
    }) %>% bind_rows %>% 
    add_column(firma = paste0('h_', 1:length(permutaciones)), .before = 1)
    salida
}
set.seed(312)
num_perms <- 20
permutaciones <- map(1:num_perms, ~ sample.int(n = nrow(dtejas_tbl)))
firmas_perms <- calc_firmas_perm(dtejas_tbl, permutaciones)
firmas_perms
```

```{r, echo = FALSE, include = FALSE}
# para nuestro ejemplo simple
perms_ejemplo <- list(perm_1, perm_2)
ejemplo_tbl <- as_tibble(mat)
names(ejemplo_tbl) <- c("doc_1", "doc_2", "doc_3", "doc_4")
calc_firmas_perm(ejemplo_tbl, perms_ejemplo)
```


---

A esta nueva matriz le llamamos **matriz de firmas** de los documentos.  La firma de un documento es una sucesión de enteros.

Cada documento se describe ahora con `r nrow(firmas_perms)` entradas,
en lugar de `r nrow(df_1)`.

Nótese que por construcción, cuando dos documentos son muy similares,
es natural que sus columnas de firmas sean similares, pues la mayor parte
de los renglones de estos dos documentos son $(0,0)$ y $(1,1)$.
Resulta ser que podemos cuantificar esta probabilidad. Tenemos el siguiente
resultado simple pero sorprendente:

```{block2, type = 'resumen'}
Sea $\pi$ una permutación escogida al azar, y $a$ y $b$ dos columnas
dadas. Entonces
$$P(f_\pi(a) = f_\pi(b)) = sim(a, b)$$
donde $sim$ es la similitud de Jaccard basada en las tejas usadas.
Sean $\pi_1, \pi_2, \ldots \pi_n$ permutaciones escogidas al azar de
manera independiente. Si $n$ es grande, entonces por la ley de los grandes números
$$sim(a,b) \approx \frac{|\pi_j : f_{\pi_j}(a) = f_{\pi_j}(b)|}{n},$$
es decir, la similitud de Jaccard es aproximadamente la proporción 
de elementos de las firmas que coinciden.
```



### Ejemplo {-}

Antes de hacer la demostración, veamos como aplicaríamos a la matriz
de firmas que calculamos arriba. Tendríamos, por ejemplo :

```{r, collapse = TRUE}
mean(firmas_perms$doc_1 == firmas_perms$doc_2)
mean(firmas_perms$doc_1 == firmas_perms$doc_3)
mean(firmas_perms$doc_3 == firmas_perms$doc_4)
```

que comparamos con las similitudes de Jaccard

```{r, collapse = TRUE}
sim_jaccard(tejas_doc[[1]], tejas_doc[[2]])
sim_jaccard(tejas_doc[[1]], tejas_doc[[3]])
sim_jaccard(tejas_doc[[4]], tejas_doc[[3]])
```

Ahora veamos qué sucede repetimos varias veces, y calculamos percentiles
de las las cantidades obtenidas en cada repetición (50) usando
20 permutaciones:

```{r, collapse = TRUE}
num_perms <- 20
firmas_rep <- map(1:50, function(i){
    perms <- map(1:num_perms, sample, x = 1:nrow(dtejas_tbl), size = nrow(dtejas_tbl))
    df_out <- calc_firmas_perm(dtejas_tbl, perms)    
    df_out$rep <- i
    df_out
})
  
map_dbl(firmas_rep, ~ mean(.x$doc_1 == .x$doc_2))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(2)
map_dbl(firmas_rep, ~ mean(.x$doc_3 == .x$doc_4))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(2)
```

Que indica que nuestro procedimiento da estimaciones razonables
de las similitudes de Jaccard.

*Observación*: si la similitud de dos documentos es cero, entonces
este procedimiento siempre da la respuesta exacta. ¿Por qué?

---

Ahora damos un argumento para demostrar este resultado.
Consideremos dos columnas $a,b$ de la matriz
de 0's y 1's, con conjuntos de tejas asociados $A,B$.

- Permutamos los reglones de las dos columnas $a$ y $b$.
- Sea $k$ la posición donde aparece el primer $(0,1)$, $(1,0)$ o $(1,1)$.
- Hay tantos renglones $(1,1)$ como elementos en $A\cap B$. Y hay tantos
renglones  $(0,1)$, $(1,0)$ o $(1,1)$ como elementos en $A\cup B$.
- Todos estos $|A\cup B|$ reglones tienen la misma probabilidad de aparecer
en la posición $k$.
- Entonces, la probabilidad condicional de que el renglón $k$ sea de tipo $(1,1)$, dado que es de algún tipo de $(1,0), (0,1), (1,1)$, es 
$$\frac{|A\cap B|}{|A\cup B|},$$
que es la similitud de Jaccard de los dos documentos.

## Cálculo de firmas por documento

Podemos evitar permutar matrices (una operación costosa) de varias formas.
Una manera de hacer esto es trabajar documento por documento, lo cual es natural para colecciones 
grandes de textos.

En este caso, es fácil ver que para una permutación dada, la firma de un documento puede encontrarse
de la siguiente forma:

```{block2, type='resumen'}
**Cálculo de la firma de un documento**

1. Para la columna $d$ (documento), tomamos todos los valores de los índices igual a 1. 
2. Aplicamos la permutación $\pi$ a cada uno de estos índices.
3. Calculamos el mínimo de los valores resultantes para obtener $f_\pi (d)$.
4. Repetimos para cada permutación para obtener la firma del documento.

```


#### Ejemplo {-}
Consideremos el documento 1 de nuestro ejemplo. Extraemos

```{r}
doc_1 <- dtejas_tbl %>% pull(doc_1)
doc_1
# extraemos ínidices iguales a 1
tejas_num <- which(doc_1 == 1)
tejas_num
```

Ahora convertimos las permutaciones a funciones:

```{r}
perms_fun <- permutaciones %>% map(~  function(z) .x[z])
```

Para calcular el primer elemento de la firma, transformamos los índices de arriba y
calculamos el mínimo:

```{r}
perms_fun[[1]](tejas_num) %>% min
```
Para el tercer elemento de la firma

```{r}
perms_fun[[3]](tejas_num) %>% min
```
Y la firma completa es
```{r}
map_int(perms_fun, ~ min(.x(tejas_num)))
```


---

Escribimos ahora una implementación para calcular tejas y calcular firmas:

```{r}
crear_tejas_num <- function(textos, k = 4){
    # las tejas serán convertidas a enteros
    num_docs <- length(textos)
    # crear tejas (codificando a números)
    tejas <- calcular_tejas(textos, k = k) %>% map(unique)
    tejas_df <- tibble(doc_id = 1:num_docs, tejas = tejas) %>%
        unnest_legacy %>% 
        mutate(tejas_num = as.numeric(factor(tejas))) %>% 
        group_by(doc_id) %>% #agrupamos las tejas de cada doc
        summarise(tejas = list(tejas_num))
    tejas_df
}
```


```{r}
calcular_firmas_doc <- function(tejas_df, hash_funs){
    # Calcula firmas por documento
    num_docs <- nrow(tejas_df)
    num_hashes <- length(hash_funs)
    tejas <- tejas_df$tejas
    firmas <- vector("list", num_docs)
    # este se puede paralelizar facilmente:
    for(i in 1:num_docs){
        firmas[[i]] <- map_dbl(hash_funs, ~ min(.x(tejas[[i]])))
    }
    tibble(doc_id = 1:num_docs, firma = firmas)
}
```

Nótese que en el código de arriba utiiizamos funciones en lugar de vectores para
representar permutaciones. Este correspondencia es como sigue:

```{r}
# convertimos vectores de permutaciones en funciones
perm_funs <- map(permutaciones, ~ function(z) .x[z])
# creamos tejas y calculamos firmas
crear_tejas_num(textos, k = 4) %>% 
  calcular_firmas_doc(perm_funs) %>%                 
  unnest_legacy %>% 
  # esta parte es para poner en forma ancha:
  group_by(doc_id) %>% 
  mutate(hash = row_number()) %>% 
  pivot_wider(names_from=doc_id, values_from = firma, names_prefix = "doc_")
```

**Observación**: nótese que no estamos permutando ningún vector o matriz en este cálculo. Esto
nos da la siguiente idea importante:


## Funciones hash

Con la técnica de firmas derivadas de permutaciones logramos
*reducir la dimensionalidad* de nuestro problema, y este es un primer paso
para encontrar pares similares en una colección grande de documentos. Sin embargo,
es necesario simular
y almacenar las distintas permutaciones (quizá usamos cientos, para estimar
con más precisión las similitudes) que vamos a utilizar. Estas permutaciones
son relativamente grandes y quizá podemos encontrar una manera más rápida de "simular"
las permutaciones.


### Ejercicio: ¿cómo sustituir permutaciones? {-}
En nuestro ejemplo anterior, tenemos $107$ tejas. Consideramos una funciones de
la forma (como se sugiere en [@mmd]):

$$h(x) = (ax + b) \bmod 107$$
donde escogemos $a$ al azar entre 1 y 106, y $b$ se escoge al azar
entre $0$ y $106$. Recuerda que $A \bmod p$ es el residuo que se obtiene al dividir $A$
entre $p$.¿Por qué es apropiada una función de este tipo?

- Demuestra primero que
la función $h(x)$ es una permutación de los enteros $\{ 0,1,\ldots, 106 \}$. Usa el
hecho de que $107$ es un número primo. 
- Si escogemos $a, b$ al azar, podemos generar distintas permutaciones.
- Esta familia de funciones no dan todas las posibles permutaciones, pero
pueden ser suficientes para nuestros propósitos, como veremos más adelante.

**Observación**: si $p$ no es primo, nuestra familia tiene el defecto de que
algunas funciones pueden nos ser permutaciones. Por ejemplo,
si 
$$h(x) = 4x + 1 \bmod 12,$$
entonces $h$ mapea el rango $\{0,1,\ldots, 11\}$ a
```{r}
h <- function(x){  (4*x +1) %% 12}
h(0:11)
```

---

Vamos a resolver nuestro problema simple usando funciones hash modulares:

```{block2, type='resumen'}
Para calcular las firmas de los documentos, en lugar de permutaciones podemos utilizar
**funciones hash modulares** (suponemos que las tejas están numeradas) de la siguiente forma:
  $$h(x) = (ax + b) \bmod p,$$
donde $p$ es un primo más grande que el número de tejas, y $a$ y $b$ se seleccionan al
azar con $0 < a, b \leq p-1$.
```

Esta función genera funciones hash modulares seleccionadas al azar:

```{r}
hash_simple <- function(p){
  # seleccionamos al azar dos enteros menores que p
  # p es un número primo
  a <- sample.int(p - 1, 2)
  hash_fun <- function(x) {
        # restamos y sumamos uno para mapear a enteros positivos
        ((a[1]*(x-1) + a[2]) %% p) + 1
    }
  hash_fun
}
set.seed(132)
hash_f <- map(1:2, ~ hash_simple(p = 107))
```

### Ejemplo {-}

Podemos examinar algunas de estas funciones:
```{r}
hash_f[[1]](1:107)
hash_f[[1]](1:107) %>% duplicated %>% any
hash_f[[2]](1:107) %>% duplicated %>% any
```

--- 

No es neceario reescribir nada en nuestro código anterior. Pero en lugar de usar permutaciones,
usaremos familias de funciones hash extraidas al azar como mostramos arriba
Reescribimos entonces nuestra función *calc_firmas* para usar las funciones
hash en lugar de permutaciones, lo cual evita simular y guardar las permutaciones.


```{r}
set.seed(2851)
hash_funs <- map(1:20, ~ hash_simple(p = 107))
# mismo codigo: creamos tejas y calculamos firmas
firmas_doc <- crear_tejas_num(textos, k = 4) %>% 
  calcular_firmas_doc(hash_funs) %>%                 
  unnest_legacy %>% 
  group_by(doc_id) %>% 
  mutate(hash = row_number()) %>% 
  pivot_wider(names_from=doc_id, values_from = firma, names_prefix = "doc_")
firmas_doc
```

Y checamos 

```{r, collapse = TRUE}
mean(firmas_doc$doc_1 == firmas_doc$doc_2)
mean(firmas_doc$doc_1 == firmas_doc$doc_3)
mean(firmas_doc$doc_3 == firmas_doc$doc_4)
```


Cuando el número de tejas no exactamente un primo, hacemos lo siguiente:

```{block2, type = 'resumen'}
**Hash de tejas numeradas**
    
 Supongamos que tenemos $\{0,1,\ldots, m-1\}$ tejas numeradas. Seleccionamos un
primo $p\geq m$, y definimos las funciones
$$H =\{ h_{a,b}(x) = (ax + b) \bmod p \}$$
En lugar de escoger una permutación al azar, escogemos
$a \in \{1,\cdots, p-1\}$, y $b \in \{0,\cdots, p-1\}$ al azar. Usamos
en el algoritmo estas funciones $h_i$ como si fueran permutaciones.
```

**Observación**: El único detalle que hay que observar aquí es que los valores hash o cubetas
a donde se mapean las tejas ya no están en el rango $\{0,1,\ldots, m-1\}$ , de manera
que no podemos interpretar como permutaciones. Pero pensando un poco vemos 
no hay ninguna razón para interpretar los valores
de $h_i(r)$ como renglones de una matriz permutadas, y no necesariamente
$h_i$ tiene que ser una permutación de los renglones. $h_i$ puede ser una
función que mapea renglones (tejas) a un rango grande de enteros, de forma
que la probabilidad de que distintos renglones sean mapeados a un mismo hash 
sea muy baja.

Obtener el minhash es simplemente encontrar el mínimo entero de los valores hash
que corresponden a tejas que aparecen en cada columna.




### Funciones hash: discusión {-}

En consecuencia, como solución para nuestro problema de minhashing podemos seleccionar un primo $p$ muy grande y utiliar la familia de congruencias $ax+b\bmod p$, seleccionando
$a$ y $b$ al azar (ver [implementación de Spark](https://github.com/apache/spark/blob/v2.1.0/mllib/src/main/scala/org/apache/spark/ml/feature/MinHashLSH.scala), donde se utiliza un primo fijo MinHashLSH.HASH_PRIME).

Más en general, podemos usar familias de funciones hash que cumplan:

- Cada teja se mapea a un entero.
- La probabilidad de que dos tejas diferentes colisionen en un mismo entero es baja.

La segunda propiedad es lo que usualmente define funciones hash apropiadas.

Otro enfoque es hacer hash directamente de las tejas (caracteres).
En este caso, buscamos una función
hash de cadenas a enteros grandes que "revuelva" las cadenas a un rango
grande de enteros. 
Es importante la calidad de la función hash, pues no queremos tener demasiadas
colisiones aún cuando existan patrones en nuestras tejas.

Por ejemplo,
podemos utilizar la función *hash_string* del paquete textreuse [@R-textreuse] (implementada
en C++):

```{r, collapse = TRUE}
textreuse::hash_string('a')
textreuse::hash_string('b')
textreuse::hash_string('El perro persigue al gato') 
textreuse::hash_string('El perro persigue al gat') 
``` 

Para obtener otras funciones hash, podemos usar una técnica distinta. Escogemos
al azar un entero, y hacemos bitwise xor con este entero al azar. 
En laa implementación de *textreuse*, por ejemplo, se hace:

```{r}
set.seed(123)
generar_hash_tr <- function(){
    r <- as.integer(stats::runif(1, -2147483648, 2147483647))
    funcion_hash <- function(x){
        bitwXor(textreuse::hash_string(x), r)    
    }
    funcion_hash
}
h_1 <- generar_hash_tr()
h_2 <- generar_hash_tr()
h_1("abcdef")
h_2("abcdef")
system.time(h_1(rep(textos, 100000)))
```

Otra opción es utilizar como función básica otra función hash estándar, como *murmur32*
o *xxhash32* (la segunda es más rápida que la primera), que también pueden
recibir semillas para obtener distintas funciones:

```{r}
set.seed(123)
generar_hash <- function(){
    r <- as.integer(stats::runif(1, 1, 2147483647))
    funcion_hash <- function(shingles){
        digest::digest2int(shingles, seed =r) 
    }
    funcion_hash
}
h_1 <- generar_hash()
h_2 <- generar_hash()
h_1("abcdef")
h_2("abcdef")
system.time(h_1(rep(textos, 100000)))
```

### Ejemplo {-}
Usando estas funciones hash de cadenas, nuestro ejemplo ser vería como sigue

```{r}
crear_tejas_str <- function(textos, k = 4){
    num_docs <- length(textos)
    tejas <- calcular_tejas(textos, k = k)
    tejas_df <- tibble(doc_id = 1:num_docs, tejas = tejas)
    tejas_df
}
```


Y ahora calculamos las firmas minhash:

```{r}
set.seed(211)
# estas funciones mapean directamente tejas a enteros:
hash_funs <- map(1:20, ~ generar_hash())
# mismo coigo creamos tejas y calculamos firmas
firmas_doc <- crear_tejas_str(textos, k = 4) %>% 
  calcular_firmas_doc(hash_funs) %>%                 
  unnest_legacy %>% 
  group_by(doc_id) %>% 
  mutate(hash = row_number()) %>% 
  pivot_wider(names_from=doc_id, values_from = firma, names_prefix = "doc_")
firmas_doc
```

Podemos estimar similitudes igual que en los ejemplos anteriores:

```{r, collapse = TRUE}
mean(firmas_doc$doc_1 == firmas_doc$doc_2)
mean(firmas_doc$doc_1 == firmas_doc$doc_3)
mean(firmas_doc$doc_3 == firmas_doc$doc_4)
```


## Ejemplo: tweets 

Ahora buscaremos tweets similares en una colección de un [dataset de
kaggle](https://www.kaggle.com/rgupta09/world-cup-2018-tweets/home?utm_medium=email&utm_source=mailchimp&utm_campaign=datanotes-20180823).

```{r}
ruta <- "../datos/FIFA.csv"
#gc_ruta <- "https://storage.cloud.google.com/metodos-analiticos/FIFA.csv"
if(!file.exists(ruta)){
    stop("Es necesario bajar manualmente el archivo de la liga de Kaggle de arriba y copiarlo a datos/FIFA.csv")
} else {
    fifa <- read_csv(ruta)
}
tw <- fifa$Tweet
tw[1:10]
```

Seleccionamos funciones hash al azar:

```{r firmas_strdoc}
set.seed(9192)
hash_f <- map(1:50, ~ generar_hash())
system.time(tejas_tbl <- crear_tejas_str(tw[1:200000], k = 5))
system.time(firmas_tw <- calcular_firmas_doc(tejas_tbl, hash_f))
```

Y ahora podemos utilizar esta representación compacta de firmas para estimar similitud.
Por ejemplo, ¿cuáles son tweets similares al primero? 

```{r}
similitud_approx <- 
  map_dbl(1:length(firmas_tw$firma), 
    ~ mean(firmas_tw$firma[[.x]] == firmas_tw$firma[[1]]))
indices <- which(similitud_approx > 0.4)
length(indices)
similares <- tibble(tweet = tw[indices],
           sim_approx = similitud_approx[indices]) %>%
    arrange(sim_approx)
DT::datatable(sample_n(similares, 50))
```

Nótese que estos podemos llamarlos más apropiadamente *candidatos* a elementos similares,
pues todavía no conocemos la similitud exacta. Esto podemos hacerlo recuperando las tejas que ya calculamos, y 
calculando la similitud exacta:

```{r}
tejas_orig <- tejas_tbl$tejas[[1]]
candidatos <- filter(tejas_tbl, doc_id %in% indices) %>% 
  mutate(similitud = map_dbl(tejas, ~ sim_jaccard(.x, tejas_orig)))
DT::datatable(candidatos %>% arrange(similitud) %>% 
                select(doc_id, similitud) %>% 
                mutate_if(is.numeric, ~ round(.x, 3)))
```

**Observación**: para hacer esta búsqueda tenemos que recorrer todos los documentos. Aunque
usamos las firmas, que son más compactas, veremos una forma más eficiente de hacer esta búsqueda.

## Buscando los pares similares

Aunque hemos reducido el trabajo para hacer comparaciones de documentos,
no hemos hecho mucho avance en encontrar todos los pares similares
de la colección completa de documentos. Intentar calcular similitud
para todos los pares (del orden $n^2$) es demasiado trabajo, incluso 
aproximando con minhashes:

```{r simstextreuse}
firmas_200 <- filter(firmas_tw, doc_id <= 1000)
system.time({
  pares <- crossing(firmas_200, 
    firmas_200 %>% rename(doc_id_1 = doc_id, firma_1 = firma)) %>%
    filter(doc_id < doc_id_1)
  pares  <- pares %>% mutate(sim = map2_dbl(firma_1, firma, ~ mean(.x == .y)))
})
pares <- pares %>% filter(sim > 0.20) %>% arrange(desc(sim))
pares
```

Nótese que si tuviéramos $10$ veces más tweets (una fracción todavía del conjunto completo) el número de comparaciones se multiplica por $100$ aproximadamente.
En la siguiente parte veremos como aprovechar estos minhashes para hacer una
búsqueda más eficiente de pares similares.

## Locality sensitive hashing (LSH) para documentos

Calcular todas las posibles similitudes de una
colección de un conjunto no tan grande de documentos es difícil. Sin
embargo, muchas veces lo que nos interesa es simplemente agrupar
colecciones de documentos que tienen alta similitud.

Una técnica resolver este problema es Locality Sensitive
Hashing (LSH), que generalizamos en la sección siguiente. Comenzamos
construyendo LSH basado en las firmas de minhash. La idea general
es: 

```{block2, type = 'resumen'}
**Idea general de Minshashing LSH**

- Recorremos la matriz de firmas documento por documento
- Asignamos el documento a una cubeta dependiendo de sus valores minhash (su firma).
- Todos los pares de documentos que caen en una misma cubeta son **candidatos** a pares similares. Generalmente tenemos mucho menos candidatos que el total de posibles pares.
- Checamos la similitud exacta todos los pares candidatos dentro de cada cubeta que contenga más de un
elemento, y filtramos si es necesario.
```



Veremos qie con este método hemos resuelto de manera aproximada nuestro problema 
de encontrar pares similares. En la siguiente parte discutiremos cómo decidir
si es o no una buena aproximación. Veremos también formas de diseñar las cubetas para obtener candidatos con la
similitud que busquemos (por ejemplo, mayor a $0.5$, mayor a $0.9$, etc.). 


### Ejemplo: cubetas de firmas completas

Calculamos los minhashes, y creamos una cubeta para cada firma minhash diferente. Los
candidatos a similitud son pares que caen en una misma cubeta. Como usamos todos
los hashes, los candidatos tienden a ser textos muy similares.

```{r}
textos_dup <- c(textos,  textos[2], "este es el Documento de Ejemplo" ,
                paste0(textos[2], '!'), 'texto diferente a todos', "un gato negro",
                "mi gato negro")
textos_dup
set.seed(21)
# podemos usar estas funciones hash
hash_f <- map(1:6, ~ generar_hash())
tejas <- crear_tejas_str(textos_dup, k = 4)
firmas <- calcular_firmas_doc(tejas, hash_f)
firmas_2 <- firmas %>% 
    # unir los minhashes en un una cadena
    mutate(cubeta = map_chr(firma, paste, collapse ="/")) %>%
    select(-firma)
firmas_2
```

Podemos hacer hash de la cadena de la cubeta:


```{r}
firmas_2 <- firmas %>% 
    mutate(cubeta = map_chr(firma, paste, collapse ="/")) %>% 
    mutate(cubeta_2 = map_chr(cubeta, digest::digest)) %>% 
    select(-firma, -cubeta)
firmas_2
```


Ahora agrupamos por cubetas:

```{r}
cubetas_df <- firmas_2 %>% group_by(cubeta_2) %>% 
    summarise(docs = list(doc_id), .groups = "drop") %>% 
    mutate(n_docs = map_int(docs, length)) 
```

y filtramos las cubetas con más de un elemento:

```{r}
cubetas_df <- cubetas_df %>% filter(n_docs > 1)
cubetas_df
```

Y los candidatos de alta similud se extraen de estas cubetas
que tienen más de $1$ elemento. Son:

```{r}
cubetas_df$docs
```

Ahora podemos extraer los pares candidatos de similitud alta. Nótese que creamos *todos* 
los pares dentro de cada cubeta:

```{r}
extraer_pares <- function(cubetas_df, cubeta, docs, textos = NULL){
   enq_cubeta <- enquo(cubeta)
   enq_docs <- enquo(docs)
   pares <- cubetas_df %>% 
    group_by(!!enq_cubeta) %>% 
    mutate(pares = map(!!enq_docs, ~ combn(sort(.x), 2, simplify = FALSE))) %>%
    select(!!enq_cubeta, pares) %>% unnest_legacy %>% 
    mutate(a = map_int(pares, 1)) %>% 
    mutate(b = map_int(pares, 2)) %>% 
    select(-pares) %>% ungroup %>% select(-!!enq_cubeta) %>% 
    unique #quitar pares repetidos
   if(!is.null(textos)){
       pares <- pares %>% mutate(texto_a = textos[a], texto_b = textos[b])
   }
   pares %>% ungroup 
}
candidatos <- extraer_pares(cubetas_df, cubeta_2, docs, textos = textos_dup)
DT::datatable(candidatos)
```



### Ejemplo: algún grupo de minhashes coincide

Si quisiéramos capturar como candidatos pares de documentos con similitud
más baja, podríamos pedir que coincidan solo algunos de los hashes. Por ejemplo,
para agrupar textos con algún grupo de $2$ minshashes iguales, podríamos hacer:

```{r}
particion <- split(1:6, ceiling(1:6 / 2))
particion
separar_cubetas_fun <- function(particion){
    function(firma){
        map_chr(particion, function(x){
            prefijo <- paste0(x, collapse = '')
            cubeta <- paste(firma[x], collapse = "/")
            paste(c(prefijo, cubeta), collapse = '|')
        })
    }
}
sep_cubetas <- separar_cubetas_fun(particion)
sep_cubetas(firmas$firma[[1]])
firmas_3 <- firmas %>% 
    mutate(cubeta = map(firma, sep_cubetas)) %>% 
    select(-firma) %>% unnest_legacy
firmas_3
```

Ahora agrupamos por cubetas:

```{r}
cubetas_df <- firmas_3 %>% group_by(cubeta) %>% 
    summarise(docs = list(doc_id), .groups = "drop") %>% 
    mutate(n_docs = map_int(docs, length)) 
```

y filtramos las cubetas con más de un elemento:

```{r}
cubetas_df <- cubetas_df %>% filter(n_docs > 1)
cubetas_df
```

```{r}
pares_candidatos <- extraer_pares(cubetas_df, cubeta, docs, textos = textos_dup) %>% 
                  arrange(texto_a)
DT::datatable(pares_candidatos)
```

Ahora podemos calcular similitud exacta y agrupar:

```{r}
pares_scores <- pares_candidatos %>% 
  mutate(score = map2_dbl(texto_a, texto_b,
  ~ sim_jaccard(calcular_tejas(.x, 5), calcular_tejas(.y, 5))))
DT::datatable(pares_scores)
```

Por ejemplo, podemos hacer clustering con las similitudes de Jaccard:

```{r}
pares_dis <- pares_scores %>% select(a, b, score) 
pares_g <- igraph::graph_from_data_frame(pares_dis, directed = FALSE)
wc <- igraph::cluster_label_prop(pares_g, weights = pares_g$score)
wc
```

---

Usando de la idea de las cubetas podemos entonces resolver eficientemente los siguientes:

```{block2, type='resumen'}
**Vecinos cercanos**
  
- Para encontrar grupos de documentos similares podemos usar las cubetas. Nótese que un documento
puede estar en varias cubetas.
- Para encontrar vecinos cercanos de un documento X,
extraemos todos los elementos de sus cubetas, y calculamos similitud de esos documentos contra el documento X. Filtramos
si es necesario para eliminar pares de similitud baja.
- Para encontrar vecinos cercanos de un nuevo documento, calculamos sus tejas, firma y finalmente cubetas. Recuperamos los elementos que caen en sus cubetas. Evaluamos similitud y filtramos si es necesario.

**Pares de similitud alta**

- Para encontrar pares de similitud alta, calculamos los pares *solo dentro de cada cubeta*. Eliminamos
pares duplicados, calculamos similitud exacta y filtramos 
si es necesario para eliminar pares de baja similitud que por azar cayeron en una misma cubeta.

**Clustering**
  
- Podemos hacer clustering con los datos de pares similares, por ejemplo, con un algoritmo de detección
de comunidades (como hicimos arriba).
```


## Ejemplo: pares similares de tweets

En primer lugar, podemos deduplicar tweets idénticos. Esto lo podemos hacer con una sola función hash: 

```{r, collapse=TRUE}
tw_hashes <- digest::digest2int(tw)
tw_dedup <- tibble(tweet = tw, hash = tw_hashes) %>% 
  group_by(hash) %>% 
  summarise(tweet = tweet[1], .groups = "drop") %>%  
  mutate(longitud = nchar(tweet)) %>% 
  filter(longitud >= 5) %>% 
  pull(tweet)
length(tw)
length(tw_dedup)
```

Calculamos firmas

```{r}
set.seed(9192)
hash_f <- map(1:12, ~ generar_hash())
system.time(tejas_tbl <- crear_tejas_str(tw_dedup, k = 5))
system.time(firmas_tw <- calcular_firmas_doc(tejas_tbl, hash_f))
```

Y ahora calculamos cubetas. En este caso, habrá dos cubetas por documento, cada una
con 6 hashes:

```{r}
particion <- split(1:12, ceiling(1:12 / 6))
particion
sep_cubetas <- separar_cubetas_fun(particion) 
sep_cubetas(firmas_tw$firma[[1]])
system.time(
  cubetas_tbl <- firmas_tw %>%
    mutate(cubeta = map(firma, sep_cubetas)) %>%
    unnest_legacy(cubeta) %>% 
    group_by(cubeta) %>% 
    summarise(docs = list(doc_id), n = length(doc_id)) %>%
    arrange(desc(n))
)
cubetas_tbl %>% arrange(desc(n)) %>% head(10)
```

Nótese que 

```{r}
cubetas_tbl$docs[[1]] %>% head %>% tw_dedup[.]
```


```{r}
cubetas_tbl$docs[[2]] %>% head %>% tw_dedup[.]
```

```{r}
cubetas_tbl$docs[[3]] %>% head %>% tw_dedup[.]
```

```{r}
cubetas_nu_tbl <- filter(cubetas_tbl, n > 2)
cubetas_nu_tbl %>% nrow()
pares_candidatos <- extraer_pares(cubetas_nu_tbl, cubeta, docs, textos = tw_dedup) %>% 
                  arrange(texto_a)
DT::datatable(pares_candidatos %>% head(100)) 
```

Ahora podemos calcular similitud exacta y agrupar. El número de pares es manejable. Filtramos
aquellas similitudes que nos interesen:

```{r}
pares_scores <- pares_candidatos %>% 
  mutate(score = map2_dbl(texto_a, texto_b,
  ~ sim_jaccard(calcular_tejas(.x, 5), calcular_tejas(.y, 5)))) %>% 
  filter(score > 0.8)
DT::datatable(pares_scores)
```

Podemos hacer clustering con las similitudes de Jaccard, filtrando los
que estén por debajo de una similitud predeterminada:

```{r}
pares_dis <- pares_scores %>% select(a, b, score)
pares_g <- igraph::graph_from_data_frame(pares_dis, directed = FALSE)
wc <- igraph::cluster_label_prop(pares_g, weights = pares_g$score)
wc
```

Y con esto hemos terminado  nuestro ejercicio de deduplicación. 

**Preguntas**:

1. ¿Puedes aproximar el tiempo que tardaría calcular similitud exacta entre todos los pares? Compara
con el proceso mostrado arriba
2. Piensa en aplicaciones interesantes del proceso que acabamos de mostrar, quizá pensando en otro tipo
de textos.
3. ¿Qué efecto tiene el número de hashes que escogimos? ¿El número de cubetas que formamos? (Discutiremos más
en la siguiente parte)

## Ejemplo: encontrar vecinos cercanos de documentos nuevos

Aplicamos la transformación a cubetas:

```{r, message=FALSE, warning=FALSE}
# encontrar firma minhash
textos_nuevos <- c("Croatia broke through to finals", "One of these will be in the final Russia Croatia Sweden Switzerland")
cubetas_nuevas <- textos_nuevos %>% 
  crear_tejas_str(., k = 5) %>%
  calcular_firmas_doc(., hash_f) %>% 
  mutate(cubeta = map(firma, sep_cubetas)) %>%
  unnest_legacy(cubeta) %>% 
  pull(cubeta)
```

```{r}
cubetas_tbl %>% filter(cubeta %in% cubetas_nuevas) %>% unnest_legacy() %>% 
  pull(docs) %>% tw_dedup[.]
```



## Resumen

Veamos qué resolvimos con cada uno de los pasos, desde la conversión a tejas hasta vecinos cercanos
[resumen adaptado de aquí](https://www.slideshare.net/MailRuGroup/okru-finding-similar-items-in-highdimensional-spaces-locality-sensitive-hashing):

1. Método de tejas: partes de documentos similares pueden ocurrir en distinto orden.
2. Firmas de minhashing: Representación de documentos en tejas puede utilizar mucha memoria. Con minhashing tenemos una versión más compacta que preserva similitud
3. LSH y cubetas: no es necesario comparar todos los pares posibles, sino solamente aquellos con firmas similares.